# Recommender System

## The Purpose

-   Pick up relevant data from a store database in order to create a recommender system.
-   The database contains informations such as libelles, products family, dates of purchase, client ids, ticket ids, etc ...
-   Explore different ways to build recommender systems
-   Manage large amount of data when processing it and when building models

## Our Solution

Given the data available, we decided to use **Collaborative Filtering**.
This way, we can use data generated by clients in order to create _clusters_ or resolve a _matrix factorization equation_.
We also had the possibility to use **Content-based Filtering** but it is usually more useful in case of a cold start problem.
The **Next Best Purchase** method could have been a solution if we had more precise data about time.

## Folder Structure

Our first solution is **clients-libelles based clustering method**. We trained our models with 3 different strategies using _Scikit-learn_:

-   KMeans/
-   DBSCAN/
-   Spectral/
    In each folder, you can find a `create_models_and_plot_clusters.ipynb` file which allowed us to preprocess the data, create the model based on the current strategy of the folder you're in, and plot the clusters we got from our training.
    In the `KMeans/` folder, you will also find a `recommender_system.ipynb` file which contains the logic to build our recommendations based on our clusters.
    The same file could be replicated to build recommendations for the DBSCAN clustering system and the Spectral clustering system.

We also provide a solution using **matrix factorization** by _Tensforflow Recommenders_, whether based on tickets-libelles relation or clients-libelles relation. Both of those solutions can be find respectively in:

-   tickets_based_tensorflow_recommenders/
-   clients_based_tensorflow_recommenders/
    In each folder, you can find a `create_tower_model_based_on_xx.ipynb` file. Its mission is to preprocess data, create the _Query Model_ et _Candidate Model_ required by _Tensorflow Recommenders_ in order to build the final _Tower Model_. Finally, we save the recommendations as vectors using the Scann library.
    You will also find `load_model_xx.ipynb` file which demonstrates our the recommendations should be served in production.
    You may also need to download these models from our google drive since we could not push them to this repository.
    The links are in the notebooks.

Our final goal was to demonstrate that our recommendations could be served very quickly in a production environment.
In the `API/` folder, you will find a simple frontend and a simple apis that prove our point.

Rest of the files in the root directory are mainly to explore the dataset.

## Preprocessing for Clustering

Our pipeline is composed of 4 major steps:

-   Encoding (labelEncoder)
-   Feature engineering (CountVectorizer)
-   Dimensionality Reduction (SVD)
-   Normalization

## Recommend with clusters

-   For each cluster, we get a set of items
-   We count the occurrences of items in a cluster and sort them (DESC)
-   We define a threshold : this will allow us to determine whether an item is relevant or not in a particular cluster
-   For a given client, we get a set of items he already bought

Now we can recommend the TOP K items given his previous purchases and the sorting of items in his corresponding cluster

## Preprocessing for Matrix Factorization

-   Convert pandas dataframe to tensorslices
-   Encoding with StringLookup and IntegerLookup
-   Prefetch data
-   Set up parallel calls
-   Specify a good batch size
-   Cache data
