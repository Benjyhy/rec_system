{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "print('Pandas version: ', pd.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print('NumPy version: ', np.__version__)\n",
    "\n",
    "import matplotlib\n",
    "print('Matplotlib version: ', matplotlib.__version__)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "print('Scikit-Learn version: ', sklearn.__version__)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "import pickle\n",
    "print('Pickle version: ', pickle.format_version)\n",
    "\n",
    "import sys\n",
    "print('Sys version: ', sys.version[0:5])\n",
    "\n",
    "from sys import exc_info\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define carts : a ticket contains one or more libelles\n",
    "carts = pd.read_csv('KaDo.csv', usecols = ['TICKET_ID', 'LIBELLE'])\n",
    "print('Shape of carts dataset is: ',carts.shape, '\\n')\n",
    "print('Number of different tickets is:', len(carts[\"TICKET_ID\"].value_counts()))\n",
    "carts.T\n",
    "carts.to_csv('all_carts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80fbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all unique tickets\n",
    "tickets = np.unique(carts['TICKET_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664eeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode libelles so we can use them in our algorithms \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(carts['LIBELLE'])\n",
    "print('Number of items is:', len(le.classes_))\n",
    "\n",
    "libelle_encoded = le.transform(carts['LIBELLE'])\n",
    "carts['LIBELLE_ENCODED'] = libelle_encoded\n",
    "\n",
    "# Save encoded libelles into csv file\n",
    "carts_encoded = carts.drop(['LIBELLE'], axis=1)\n",
    "carts_encoded.to_csv('all_carts_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of strings\n",
    "# Each string corresponds to the items that are in the ticket (=cart)\n",
    "def itemsListForTickets(tickets, tickets_data):\n",
    "    tickets_items_list = []\n",
    "    for ticket in tickets:\n",
    "        tickets_items_list.append(str(list(tickets_data[tickets_data['TICKET_ID'] == ticket]['LIBELLE_ENCODED'])).split('[')[1].split(']')[0])\n",
    "    return tickets_items_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c280f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fonction using list of unique tickers and the carts_endoded df\n",
    "tickets_items_list = itemsListForTickets(tickets, carts_encoded)\n",
    "print('Items list for', len(carts), ' tickets')\n",
    "print('A list of first 10 tickets bought items: \\n', tickets_items_list[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize each string and return all vectors and feature names\n",
    "def prepSparseMatrix(list_of_str):\n",
    "    cv = CountVectorizer(token_pattern = r'[^\\,\\ ]+', lowercase = False)\n",
    "    sparseMatrix = cv.fit_transform(list_of_str)\n",
    "    return sparseMatrix, cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseMatrix, feature_names = prepSparseMatrix(tickets_items_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1260e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each ticket, 0 if item is not in the cart, 1 if items is not in the cart\n",
    "df_sparseMatrix = pd.DataFrame.sparse.from_spmatrix(sparseMatrix, index = tickets, columns = feature_names)\n",
    "df_sparseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification step : makek sure 0 and 1 are correctly distributed \n",
    "# in the sparse matrix according to the first 6 tickets initial DF\n",
    "first_6_tickets_SM = carts_encoded[carts_encoded['TICKET_ID'].isin(tickets[:6])].sort_values('TICKET_ID')\n",
    "print(first_6_tickets_SM.T)\n",
    "print(df_sparseMatrix.loc[np.unique(first_6_tickets_SM['TICKET_ID']), list(map(str, np.unique(first_6_tickets_SM['LIBELLE_ENCODED'])))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d866b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use elbow method to define the optimized K\n",
    "# WCSS = Within Clusters Sum of Squares\n",
    "class elbowMethod():\n",
    "    def __init__(self, sparseMatrix):\n",
    "        self.sparseMatrix = sparseMatrix\n",
    "        self.wcss = list()\n",
    "        self.differences = list()\n",
    "    def run(self, init, upto, max_iterations = 300):\n",
    "        for i in range(init, upto + 1):\n",
    "            kmeans = KMeans(n_clusters=i, init = 'k-means++', max_iter = max_iterations, n_init = 10, random_state = 0)\n",
    "            kmeans.fit(sparseMatrix)\n",
    "            self.wcss.append(kmeans.inertia_)\n",
    "        self.differences = list()\n",
    "        for i in range(len(self.wcss)-1):\n",
    "            self.differences.append(self.wcss[i] - self.wcss[i+1])\n",
    "    def showPlot(self, boundary = 500, upto_cluster = None):\n",
    "        if upto_cluster is None:\n",
    "            WCSS = self.wcss\n",
    "            DIFF = self.differences\n",
    "        else:\n",
    "            WCSS = self.wcss[:upto_cluster]\n",
    "            DIFF = self.differences[:upto_cluster - 1]\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.subplot(121).set_title('Elbow Method Graph')\n",
    "        plt.plot(range(1, len(WCSS) + 1), WCSS)\n",
    "        plt.grid(b = True)\n",
    "        plt.subplot(122).set_title('Differences in Each Two Consective Clusters')\n",
    "        len_differences = len(DIFF)\n",
    "        X_differences = range(1, len_differences + 1)\n",
    "        plt.plot(X_differences, DIFF)\n",
    "        plt.plot(X_differences, np.ones(len_differences)*boundary, 'r')\n",
    "        plt.plot(X_differences, np.ones(len_differences)*(-boundary), 'r')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate elbowMethod with our SM\n",
    "elbow_method = elbowMethod(sparseMatrix) \n",
    "# Test SM with several K\n",
    "elbow_method.run(1, 10)\n",
    "elbow_method.showPlot(boundary = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fe720",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method.run(11, 30)\n",
    "elbow_method.showPlot(boundary = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da22e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Data on Model and saving it with pickle (kmeans)\n",
    "kmeans = KMeans(n_clusters=8, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "kmeans.fit(sparseMatrix)\n",
    "pickle.dump(kmeans, open('model_kmeans.pkl', 'wb'))\n",
    "clusters = kmeans.fit_predict(sparseMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ffa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Data on Model and saving it with pickle (dbscan)\n",
    "dbscan = DBSCAN(eps=3, min_samples=2)\n",
    "dbscan.fit(sparseMatrix)\n",
    "pickle.dump(dbscan, open('model_dbscan.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24eb93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Data on Model and saving it with pickle (spectral)\n",
    "spectral = SpectralClustering(n_clusters=2, assign_labels='discretize', random_state=0)\n",
    "spectral.fit(sparseMatrix)\n",
    "pickle.dump(spectral, open('model_spectral.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635cf360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display each ticket and its corresponding cluster\n",
    "tickets_cluster = pd.DataFrame(np.concatenate((tickets.reshape(-1,1), clusters.reshape(-1,1)), axis = 1), columns = ['TICKET_ID', 'Cluster'])\n",
    "tickets_cluster.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, how many times do we find each libelle\n",
    "def clustersLibelles(tickets_cluster, tickets_data):\n",
    "    clusters = list(tickets_cluster['Cluster'])\n",
    "    each_cluster_libelles = list()\n",
    "    for i in range(len(np.unique(clusters))):\n",
    "        tickets_list = list(tickets_cluster[tickets_cluster['Cluster'] == i]['TICKET_ID'])\n",
    "        tickets_libelles_list = list()\n",
    "        for ticket in tickets_list:    \n",
    "            tickets_libelles_list.extend(list(tickets_data[tickets_data['TICKET_ID'] == ticket]['LIBELLE_ENCODED']))\n",
    "        tickets_libelles_counts = list()\n",
    "        tickets_libelles_counts.extend([[libelle, tickets_libelles_list.count(libelle)] for libelle in np.unique(tickets_libelles_list)])\n",
    "        each_cluster_libelles.append(pd.DataFrame(tickets_libelles_counts, columns=['LIBELLE_ENCODED', 'Count']).sort_values(by = ['Count'], ascending = False).reset_index(drop=True))\n",
    "    return each_cluster_libelles\n",
    "\n",
    "cluster_libelles = clustersLibelles(tickets_cluster, carts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b66eada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For cluster 1, libelle [LIBELLE_ENCODED] is present [Count] times\n",
    "cluster_libelles[1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different tickets are in each cluster ?\n",
    "# Also check that the sum of the nb of tickets in each cluster == the total number of tickets at start\n",
    "total_nb_tickets = 0\n",
    "for i in range(8):\n",
    "    len_tickets = tickets_cluster[tickets_cluster['Cluster'] == i].shape[0]\n",
    "    total_nb_tickets += len_tickets\n",
    "    print('Tickets in Cluster ' + str(i) + ' -> ', len_tickets) \n",
    "print(f\"Total number of tickets : {total_nb_tickets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the tickets belonging to a certain client (by his id)\n",
    "tickets_of_clients = pd.read_csv('KaDo.part', usecols = ['TICKET_ID', 'CLI_ID'])\n",
    "\n",
    "def tickets_of_a_client(client_id):\n",
    "    tickets_df = tickets_of_clients[tickets_of_clients['CLI_ID'] == client_id]\n",
    "    return tickets_df['TICKET_ID'].unique()    \n",
    "    \n",
    "analyzed_client_tickets = tickets_of_a_client(941958669)\n",
    "analyzed_client_tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the clusters of a specific client thanks to his former tickets\n",
    "def clusters_of_a_client(tickets):\n",
    "    client_clusters = list()\n",
    "    for ticket_id in tickets:\n",
    "        current_cluster = tickets_cluster[tickets_cluster['TICKET_ID'] == ticket_id]['Cluster'].values[0];\n",
    "        if current_cluster not in client_clusters:\n",
    "            client_clusters.append(current_cluster)\n",
    "    return client_clusters\n",
    "\n",
    "analyzed_client_clusters = clusters_of_a_client(analyzed_client_tickets)\n",
    "analyzed_client_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e47d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bought_items(tickets):\n",
    "    bought_items = list()\n",
    "    for ticket_id in tickets:\n",
    "        libelles_serie = df_sparseMatrix.loc[ticket_id]\n",
    "        for item_id in list(libelles_serie[libelles_serie==1].index):\n",
    "            bought_items.append(item_id)\n",
    "    return list(map(int,bought_items))\n",
    "            \n",
    "analyzed_client_bought_items = bought_items(analyzed_client_tickets)\n",
    "analyzed_client_bought_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_contained_in_clusters(clusters):\n",
    "    items_contained_in_clusters = list()\n",
    "    for cluster in clusters:\n",
    "        items = cluster_libelles[cluster][cluster_libelles[cluster]['Count'] > 500]['LIBELLE_ENCODED'].values\n",
    "        for item in items:\n",
    "            if item not in items_contained_in_clusters:\n",
    "                items_contained_in_clusters.append(item)\n",
    "    return items_contained_in_clusters\n",
    "\n",
    "items_in_clusters = items_contained_in_clusters(analyzed_client_clusters)\n",
    "items_in_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d52a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_suggestion(bought_items, items_in_clusters):\n",
    "    suggested_items = list()\n",
    "    for item in items_in_clusters:\n",
    "        if item not in bought_items:\n",
    "            suggested_items.append(item)\n",
    "    return suggested_items\n",
    "\n",
    "suggested_items = items_suggestion(analyzed_client_bought_items, items_in_clusters)\n",
    "suggested_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode labels so we can read the\n",
    "decoded_suggested_items.append(le.inverse_transform(si))\n",
    "decoded_suggested_items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
